#!/usr/bin/env python3
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from csv import writer
from os import mkdir
from os.path import isdir, isfile, join
from svm import load_observations, split_data, measure_performance
from time import time

train_pct = .8
observation_batches = 1
algorithm = 'svm'
input_observations = 'gen/observations'
dump_directory = 'models'
svm_models_path = 'svm_models.csv'
nn_models_path = 'nn_models.csv'
parser = ArgumentParser(description='Fits Machine Learning models onto the training datasets and outputs performance',
                        formatter_class=ArgumentDefaultsHelpFormatter)
parser.add_argument('-a', '--algorithm', type=str, default=algorithm, help='The algorithm to use',
                    choices=['nn', 'svm', 'nb'])
parser.add_argument('-b', '--batches', type=int, default=observation_batches, help='The amount of observation batches '
                                                                                   'to use. Be careful with large '
                                                                                   'numbers!')
parser.add_argument('-i', '--input', type=str, default=input_observations, help='The directory which contains the '
                                                                                'input training and test observations '
                                                                                '(output of `gen_vectors.py`).')

args = parser.parse_args()
algorithm = args.algorithm
observation_batches = args.batches
input_observations = args.input
if not isdir(dump_directory):
    mkdir(dump_directory)
x = load_observations(observation_batches, input_observations)
x_tr, x_te, y_tr, y_te = split_data(x, train_pct)
del x
if algorithm == 'svm':
    print('Using support vector machine')
    from sklearn import svm
    from sklearn.externals.joblib import dump
    class_weights = {0: 1, 1: 2}
    s = svm.LinearSVC(tol=1e-5, C=1.5, class_weight=class_weights, max_iter=2000, verbose=1)  # ToDo: Kernel trick (binary features)
    s.fit(x_tr, y_tr)
    tpr, fpr = measure_performance(s.predict(x_te), y_te)
    print('True positive rate', tpr, 'False positive rate', fpr)
    id = str(int(time()))
    if isfile(svm_models_path):
        with open(svm_models_path, 'a') as file:
            models_writer = writer(file)
            models_writer.writerow([id, tpr, fpr, s.C, s.tol, s.class_weight[0], s.class_weight[1], s.max_iter])
    else:
        with open(svm_models_path, 'w') as file:
            models_writer = writer(file)
            models_writer.writerow(['id', 'tpr', 'fpr', 'C', 'tol', 'positive_class_weight', 'negative_class_weights', 'max_iter'])
            models_writer.writerow([id, tpr, fpr, s.C, s.tol, s.class_weight[0], s.class_weight[1], s.max_iter])
    if not isdir(join(dump_directory, algorithm)):
        mkdir(join(dump_directory, algorithm))
    dump(s, join(dump_directory, algorithm, id))
elif algorithm == 'nn':
    print('Using neural network')
    from keras.models import Sequential
    from keras.layers import Dense, Activation
    from keras.optimizers import Adam
    from matplotlib import pyplot as plt
    model = Sequential()
    model.add(Dense(500000, input_dim=len(x_tr[0])))
    model.add(Activation('relu'))
    model.add(Dense(10000))
    model.add(Activation('relu'))
    model.add(Dense(1000))
    model.add(Activation('relu'))
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dense(1))
    model.add(Activation('softmax'))
    adam = Adam(lr=1e-4, beta_1=.9, beta_2=.999, epsilon=1e-8, decay=.0)
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['binary_accuracy'])
    history = model.fit(x_tr, y_tr, epochs=100, batch_size=8)
    loss_and_metrics = model.evaluate(x_te, y_te)
    print(loss_and_metrics)
    tpr, fpr = measure_performance(model.predict(x_te), y_te)
    print('True positive rate', tpr, 'False positive rate', fpr)
    id = str(int(time()))
    model.save(join(dump_directory, algorithm, id))
    if isfile(nn_models_path):
        with open(nn_models_path, 'a') as file:
            models_writer = writer(file)
            models_writer.writerow([id, tpr, fpr])
    else:
        with open(nn_models_path, 'w') as file:
            models_writer = writer(file)
            models_writer.writerow(['id', 'tpr', 'fpr'])
            models_writer.writerow([id, tpr, fpr])
    plt.plot(history.history['loss'])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.show()
    plt.plot(history.history['acc'])
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.show()
elif 'nb':
    print('Using Naive Bayes')
    from sklearn.naive_bayes import BernoulliNB
    from sklearn.externals.joblib import dump
    gnb = BernoulliNB()
    gnb.fit(x_tr, y_tr)
    tpr, fpr = measure_performance(gnb.predict(x_te), y_te)
    print('True positive rate', tpr, 'False positive rate', fpr)
# ToDo: Decision tree, Random Forest
