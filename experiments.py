#!/usr/bin/env python3
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
from csv import writer
from os import mkdir
from os.path import isdir, isfile, join
from helpers import NNData, load_observations, split_data, measure_performance
from time import time

train_pct = .8
observation_batches = 1
runs = 1
algorithms = ['svm']
input_observations = 'gen/observations'
dump_directory = 'models'
svm_models_path = 'svm_models.csv'
nn_models_path = 'nn_models.csv'
rf_models_path = 'rf_models.csv'
lr_models_path = 'lr_models.csv'
parser = ArgumentParser(description='Fits Machine Learning models onto the training datasets and outputs performance',
                        formatter_class=ArgumentDefaultsHelpFormatter)
parser.add_argument('-a', '--algorithms', type=str, nargs='*', default=algorithms, help='The algorithms to use.',
                    choices=['nn', 'svm', 'nb', 'rf', 'lr'])
parser.add_argument('-b', '--batches', type=int, default=observation_batches, help='The amount of observation batches '
                                                                                   'to use. Be careful with large '
                                                                                   'numbers!')
parser.add_argument('-d', '--data', type=str, default=input_observations, help='The directory which contains the '
                                                                                'input training and test observations '
                                                                                '(output of `gen_vectors.py`).')
parser.add_argument('-r', '--runs', type=int, default=runs, help='Number of iterations of the training process.'
                                                                       'A number larger than 1 might diminish the'
                                                                       'influence of random initialization on model'
                                                                       'results.')
parser.add_argument('-w', '--weight', type=int, default=2, help='Class weight for malware samples.')

args = parser.parse_args()
algorithms = args.algorithms
observation_batches = args.batches
input_observations = args.data
runs = args.runs
if not isdir(dump_directory):
    mkdir(dump_directory)

if 'nn' in algorithms:
    print('Using neural network')
    from keras.models import Sequential
    from keras.layers import Dense, Activation
    from keras.optimizers import Adam
    from matplotlib import pyplot as plt

    data = NNData(input_observations)

    model = Sequential()
    model.add(Dense(50000, input_dim=data.get_input_dim()))
    model.add(Activation('relu'))
    model.add(Dense(10000))
    model.add(Activation('relu'))
    model.add(Dense(1000))
    model.add(Activation('relu'))
    model.add(Dense(100))
    model.add(Activation('relu'))
    model.add(Dense(1))
    model.add(Activation('softmax'))
    adam = Adam(lr=1e-4, beta_1=.9, beta_2=.999, epsilon=1e-8, decay=.0)
    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['binary_accuracy'])

    #history = model.fit_generator(data.training_generator(observation_batches), steps_per_epoch=observation_batches, epochs=30)
    for epoch in range(30):
        for batch in data.training_generator(observation_batches):
            history = model.fit(batch[0], batch[1])
    x_te, y_te = data.get_test_data()
    loss_and_metrics = model.evaluate(x_te, y_te)
    print(loss_and_metrics)
    tpr, fpr = measure_performance(model.predict(x_te), y_te)
    print('True positive rate', tpr, 'False positive rate', fpr)
    id = str(int(time()))
    model.save(join(dump_directory, 'nn', id))
    if isfile(nn_models_path):
        with open(nn_models_path, 'a') as file:
            models_writer = writer(file)
            models_writer.writerow([id, tpr, fpr])
    else:
        with open(nn_models_path, 'w') as file:
            models_writer = writer(file)
            models_writer.writerow(['id', 'tpr', 'fpr'])
            models_writer.writerow([id, tpr, fpr])
    plt.plot(history.history['loss'])
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.show()
    plt.plot(history.history['acc'])
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.savefig(id + '.png')
else:
    from sklearn.externals.joblib import dump
    print('Loading', observation_batches, 'batches from', input_observations, '...')
    x = load_observations(observation_batches, input_observations, mode='sample')
    print('Splitting data in training and test dataset', train_pct)
    x_tr, x_te, y_tr, y_te = split_data(x, train_pct)
    del x
    class_weights = {0: 1, 1: args.weight}
    if 'svm' in algorithms:
        print('Using support vector machine')
        from sklearn import svm
        s = svm.LinearSVC(C=0.5, class_weight=class_weights, max_iter=1e6, verbose=1)
        s.fit(x_tr, y_tr)
        tpr, fpr = measure_performance(s.predict(x_te), y_te)
        print('True positive rate', tpr, 'False positive rate', fpr)
        id = str(int(time()))
        if isfile(svm_models_path):
            with open(svm_models_path, 'a') as file:
                models_writer = writer(file)
                models_writer.writerow([id, tpr, fpr, s.C, s.tol, s.class_weight[0], s.class_weight[1], s.max_iter])
        else:
            with open(svm_models_path, 'w') as file:
                models_writer = writer(file)
                models_writer.writerow(['id', 'tpr', 'fpr', 'C', 'tol', 'positive_class_weight', 'negative_class_weights', 'max_iter'])
                models_writer.writerow([id, tpr, fpr, s.C, s.tol, s.class_weight[0], s.class_weight[1], s.max_iter])
        if not isdir(join(dump_directory, 'svm')):
            mkdir(join(dump_directory, 'svm'))
        dump(s, join(dump_directory, 'svm', id))
    elif 'nb' in algorithms:
        print('Using Naive Bayes')
        from sklearn.naive_bayes import BernoulliNB
        gnb = BernoulliNB()
        gnb.fit(x_tr, y_tr)
        tpr, fpr = measure_performance(gnb.predict(x_te), y_te)
        print('True positive rate', tpr, 'False positive rate', fpr)
    elif 'rf' in algorithms:
        print('Using Random forest')
        from sklearn.ensemble import RandomForestClassifier
        rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, verbose=1, class_weight=class_weights)
        rf.fit(x_tr, y_tr)
        tpr, fpr = measure_performance(rf.predict(x_te), y_te)
        print('True positive rate', tpr, 'False positive rate', fpr)
        id = str(int(time()))
        if isfile(rf_models_path):
            with open(rf_models_path, 'a') as file:
                models_writer = writer(file)
                models_writer.writerow([id, tpr, fpr, rf.n_estimators, rf.class_weight[0], rf.class_weight[1]])
        else:
            with open(rf_models_path, 'w') as file:
                models_writer = writer(file)
                models_writer.writerow(
                    ['id', 'tpr', 'fpr', 'n_estimators', 'class_weight_0', 'class_weight_1'])
                models_writer.writerow([id, tpr, fpr, rf.n_estimators, rf.class_weight[0], rf.class_weight[1]])
        if not isdir(join(dump_directory, 'rf')):
            mkdir(join(dump_directory, 'rf'))
        dump(rf, join(dump_directory, 'rf', id))
    elif 'lr' in algorithms:
        print('Using Logistic Regression')
        from sklearn.linear_model import LogisticRegression
        lr = LogisticRegression(solver='saga', max_iter=1000, n_jobs=-1, verbose=1, class_weight=class_weights)
        lr.fit(x_tr, y_tr)
        tpr, fpr = measure_performance(lr.predict(x_te), y_te)
        print('True positive rate', tpr, 'False positive rate', fpr)
        id = str(int(time()))
        if isfile(lr_models_path):
            with open(lr_models_path, 'a') as file:
                models_writer = writer(file)
                models_writer.writerow([id, tpr, fpr, lr.dual, lr.solver, lr.C, lr.tol, lr.max_iter, lr.class_weight[0],
                                        lr.class_weight[1]])
        else:
            with open(lr_models_path, 'w') as file:
                models_writer = writer(file)
                models_writer.writerow(
                    ['id', 'tpr', 'fpr', 'dual', 'solver', 'C', 'tol', 'max_iter', 'class_weight_0', 'class_weight_1'])
                models_writer.writerow([id, tpr, fpr, lr.dual, lr.solver, lr.C, lr.tol, lr.max_iter, lr.class_weight[0],
                                        lr.class_weight[1]])
        if not isdir(join(dump_directory, 'lr')):
            mkdir(join(dump_directory, 'lr'))
        dump(lr, join(dump_directory, 'lr', id))
